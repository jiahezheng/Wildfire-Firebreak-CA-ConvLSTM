{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"muUUsUl5Brg3"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","import sys\n","import math\n","import random\n","import copy\n","\n","import pandas as pd\n","import numpy as np\n","\n","from tqdm import tqdm\n","import time\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","from matplotlib import animation as animation\n","\n","from PIL import Image\n","import matplotlib as mpl\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","\n","!pip install torchmetrics\n","from torchmetrics.image import StructuralSimilarityIndexMeasure\n","\n","from PIL import Image, ImageSequence\n","from skimage.transform import resize\n","\n","from skimage.metrics import structural_similarity as ssim\n","\n","directory = ''  # your directory\n"]},{"cell_type":"code","source":["# Define the default colormap and norm for the input, target, and prediction\n","default_cmap = mpl.colors.ListedColormap(['orange', 'yellow', 'green', 'black', 'blue', 'purple'])\n","\n","# default_bounds = [0.0, 0.02, 0.27, 0.8, 1.99, 2.8, 13.5] # for chimney\n","default_bounds = [0.0, 0.2, 0.5, 0.99, 2.45, 2.99, 13.8] # for bear & ferguson\n","\n","default_norm = mpl.colors.BoundaryNorm(default_bounds, default_cmap.N)"],"metadata":{"id":"kuFq_OEPxVWk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7SpcOdFpr91Q"},"outputs":[],"source":["VERBOSE = False"]},{"cell_type":"markdown","metadata":{"id":"lDX48lLU5pBK"},"source":["#### import data training & testing data ferguson\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grLhY9Ji5pBU"},"outputs":[],"source":["forest = Image.open(directory + 'Data/land_data_CA/Ferguson/canopy_Ferguson_2018.tif')\n","forest = np.array(forest)\n","forest[forest<-999.] = 0.\n","forest = forest/np.max(forest)\n","\n","forest = resize(forest, (128, 128))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qN8EUGT15pBU"},"outputs":[],"source":["# # # ca simulation data\n","# forest_fer_train = np.load(directory + 'Data/training_data/forest_fer_rand_ig_rand_wind_rand_tilt_4_ptbar_ml_state.npy')\n","# forest_fer_val_4pt = np.load(directory + 'Data/training_data/forest_fer_rand_ig_rand_wind_rand_tilt_4_ptbar_val_ml_state.npy')\n","\n","# # forest_fer_test_nobar = np.load(directory + 'Data/test_data/forest_fer_rand_ig_rand_wind_rand_no_ptbar_test_ml_state.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqkERmQqu2Xo"},"outputs":[],"source":["forest_fer_test_4pt = np.load(directory + 'Data/test_data/forest_fer_rand_ig_rand_wind_rand_tilt_4_ptbar_test_ml_state.npy')\n","forest_fer_test_3t = np.load(directory + 'Data/test_data/forest_fer_rand_ig_rand_wind_rand_tilt_3t_test_ml_state.npy')\n","forest_fer_test_2p = np.load(directory + 'Data/test_data/forest_fer_rand_ig_rand_wind_rand_tilt_2p_test_ml_state.npy')\n","forest_fer_test_no_bar = np.load(directory + 'Data/test_data/forest_fer_rand_ig_rand_wind_rand_tilt_no_bar_test_ml_state.npy')\n","forest_fer_test_2p_longtime = np.load(directory + 'Data/test_data/forest_ferguson_rand_ig_rand_wind_rand_tilt_2p_CA_test_long_time_ml_state.npy')"]},{"cell_type":"markdown","metadata":{"id":"M-_eLPDou23L"},"source":["#### import data training & testing data bear\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_IECJV78u23L"},"outputs":[],"source":["forest = Image.open(directory + 'Data/land_data_CA/Bear/canopy_Bear_2020.tif')\n","forest = np.array(forest)\n","forest[forest<-999.] = 0.\n","forest = forest/np.max(forest)\n","\n","forest = resize(forest, (128, 128))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFOhI7pKKfCx"},"outputs":[],"source":["forest_bear_test_4pt = np.load(directory + 'Data/test_data/forest_bear_rand_ig_rand_wind_rand_tilt_4_ptbar_test_ml_state.npy')\n","forest_bear_test_3t = np.load(directory + 'Data/test_data/forest_bear_rand_ig_rand_wind_rand_tilt_3t_test_ml_state.npy')\n","forest_bear_test_2p = np.load(directory + 'Data/test_data/forest_bear_rand_ig_rand_wind_rand_tilt_2p_test_ml_state.npy')\n","forest_bear_test_no_bar = np.load(directory + 'Data/test_data/forest_bear_rand_ig_rand_wind_rand_tilt_no_bar_test_ml_state.npy')\n","forest_bear_test_2p_longtime = np.load(directory + 'Data/test_data/forest_bear_rand_ig_rand_wind_rand_tilt_2p_CA_test_long_time_ml_state.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmM_bXZ2u23L"},"outputs":[],"source":["# #ca simulation data\n","forest_bear_train = np.load(directory + 'Data/training_data/forest_bear_rand_ig_rand_wind_rand_tilt_4_ptbar_ml_state.npy')\n","forest_bear_val_4pt = np.load(directory + 'Data/training_data/forest_bear_rand_ig_rand_wind_rand_tilt_4_ptbar_val_ml_state.npy')\n","# forest_fer_test_nobar = np.load(directory + 'Data/test_data/forest_bear_rand_ig_rand_wind_rand_no_ptbar_test_ml_state.npy')"]},{"cell_type":"markdown","metadata":{"id":"L73y0t1k1Q8t"},"source":["#### import data training & testing data chimney\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2ttXSbwJQoO"},"outputs":[],"source":["forest = Image.open(directory + 'Data/land_data_CA/Chimney/canopy_Chimney_2016.tif')\n","forest = np.array(forest)\n","forest[forest<-999.] = 0.\n","\n","forest = forest/np.max(forest)\n","forest = resize(forest, (128, 128))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhC1wQ6yrNwe"},"outputs":[],"source":["# # #ca simulation train data\n","# forest_chimney_train = np.load(directory + 'Data/training_data/forest_chimney_rand_ig_rand_wind_rand_tilt_4_ptbar_ml_state.npy')\n","# forest_chimney_val_4pt = np.load(directory + 'Data/training_data/forest_chimney_rand_ig_rand_wind_rand_tilt_4_ptbar_val_ml_state.npy')\n","# # forest_chimney_test_nobar = np.load(directory + 'Data/test_data/forest_chimney_rand_ig_rand_wind_rand_no_ptbar_test_ml_state.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CC6eYsxFFbzs"},"outputs":[],"source":["#ca simulation test data\n","forest_chimney_test_4pt = np.load(directory + 'Data/test_data/forest_chimney_rand_ig_rand_wind_rand_tilt_4_ptbar_test_ml_state.npy')\n","forest_chimney_test_3t = np.load(directory + 'Data/test_data/forest_chimney_rand_ig_rand_wind_rand_tilt_3t_test_ml_state.npy')\n","forest_chimney_test_2p = np.load(directory + 'Data/test_data/forest_chimney_rand_ig_rand_wind_rand_tilt_2p_test_ml_state.npy')\n","forest_chimney_test_no_bar = np.load(directory + 'Data/test_data/forest_chimney_rand_ig_rand_wind_rand_tilt_no_bar_test_ml_state.npy')"]},{"cell_type":"markdown","metadata":{"id":"TMxAZF9_1ace"},"source":["#### model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VZ5DUKCBcSH"},"outputs":[],"source":["class FireDataset(Dataset):\n","    def __init__(self, fire_data, seq_len):\n","        self.fire_data = fire_data\n","        shape = self.fire_data.shape\n","\n","        self.number = shape[0]\n","        self.time_step = shape[1]\n","        self.frame_len = shape[2]\n","        self.seq_len = seq_len\n","\n","    def __len__(self):\n","        return self.number* (self.time_step - self.seq_len +1)\n","\n","    def __getitem__(self, idx):\n","        idx0 = idx//(self.time_step - self.seq_len +1)\n","        idx1 = idx%(self.time_step - self.seq_len +1)\n","        input, target = torch.tensor(self.fire_data[idx0][idx1:idx1+3]),  torch.tensor(self.fire_data[idx0][idx1+3:idx1+6])\n","\n","        return  torch.stack((input, input), dim=1), target\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGxL-ZPvBerW"},"outputs":[],"source":["class ConvLSTMCell(nn.Module):\n","  def __init__(self, input_dim, hidden_dim, kernel_size):\n","    super(ConvLSTMCell, self).__init__()\n","\n","    self.input_dim = input_dim\n","    self.hidden_dim = hidden_dim\n","    self.kernel_size = kernel_size\n","\n","    padding = kernel_size // 2\n","    self.conv = nn.Conv2d(in_channels=input_dim + hidden_dim,\n","                out_channels=4 * hidden_dim,\n","                kernel_size=kernel_size,\n","                padding=padding,\n","                bias=True)\n","\n","  def forward(self, input_tensor, cur_state):\n","    h_cur, c_cur = cur_state\n","\n","    combined = torch.cat([input_tensor, h_cur], dim=1)\n","    combined_conv = self.conv(combined)\n","\n","    cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n","\n","    i = torch.sigmoid(cc_i)\n","    f = torch.sigmoid(cc_f)\n","    o = torch.sigmoid(cc_o)\n","    g = torch.tanh(cc_g)\n","\n","    c_next = f * c_cur + i * g\n","    h_next = o * torch.tanh(c_next)\n","\n","    return h_next, c_next\n","\n","  def init_hidden(self, batch_size, image_size):\n","    height, width = image_size\n","    return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n","        torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKKe4ZNBBem9"},"outputs":[],"source":["class EncoderDecoderConvLSTM(nn.Module):\n","  def __init__(self, input_dim, hidden_dim):\n","    super(EncoderDecoderConvLSTM, self).__init__()\n","\n","    self.encoder_1_convlstm = ConvLSTMCell(input_dim=input_dim, hidden_dim=hidden_dim, kernel_size=3)\n","    self.encoder_2_convlstm = ConvLSTMCell(input_dim=hidden_dim, hidden_dim=hidden_dim, kernel_size=3)\n","    self.decoder_1_convlstm = ConvLSTMCell(input_dim=hidden_dim, hidden_dim=hidden_dim, kernel_size=3)\n","    self.decoder_2_convlstm = ConvLSTMCell(input_dim=hidden_dim, hidden_dim=hidden_dim, kernel_size=3)\n","    self.decoder_CNN = nn.Conv3d(in_channels=hidden_dim, out_channels = 16 , kernel_size=(1, 3, 3), padding=(0, 1, 1))\n","\n","\n","  def autoencoder(self, x, seq_len, future_step, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4):\n","\n","    outputs = []\n","\n","    # encoder\n","    for t in range(seq_len):\n","      h_t, c_t = self.encoder_1_convlstm(input_tensor=x[:, t, :, :], cur_state=[h_t, c_t])  # we could concat to provide skip conn here\n","      h_t2, c_t2 = self.encoder_2_convlstm(input_tensor=h_t, cur_state=[h_t2, c_t2])  # we could concat to provide skip conn here\n","\n","    # encoder_vector\n","    encoder_vector = h_t2\n","\n","    # decoder\n","    for t in range(future_step):\n","      h_t3, c_t3 = self.decoder_1_convlstm(input_tensor=encoder_vector, cur_state=[h_t3, c_t3])  # we could concat to provide skip conn here\n","      h_t4, c_t4 = self.decoder_2_convlstm(input_tensor=h_t3, cur_state=[h_t4, c_t4])  # we could concat to provide skip conn here\n","      encoder_vector = h_t4\n","      outputs += [h_t4]  # predictions\n","\n","    outputs = torch.stack(outputs, 1)\n","    outputs = outputs.permute(0, 2, 1, 3, 4)\n","    outputs = self.decoder_CNN(outputs)\n","    #outputs = outputs.permute(0, 2, 1, 3, 4)\n","\n","    return outputs\n","\n","  def forward(self, x, future_seq=3, hidden_state=None):\n","\n","    # find size of different input dimensions\n","    b, seq_len, _, h, w = x.size()\n","\n","    # initialize hidden states\n","    h_t, c_t = self.encoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n","    h_t2, c_t2 = self.encoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n","    h_t3, c_t3 = self.decoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n","    h_t4, c_t4 = self.decoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n","\n","    # autoencoder forward\n","    outputs = self.autoencoder(x, seq_len, future_seq, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4)\n","\n","    return outputs\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Q3hvTHvBef9"},"outputs":[],"source":["def train_convLSTM(train_data, test=False, test_data=None):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    input_dim = 2\n","    hidden_dim = 128\n","    kernel_size = 3\n","    seq_len = 3\n","    model = EncoderDecoderConvLSTM(input_dim, hidden_dim).to(device)\n","\n","    criterion = nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)\n","    criterion2 = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n","\n","    batch_size = 16\n","    iter = 5\n","\n","    fire_data_loader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","    if test:\n","        train_loss_list = []\n","        mse_list = []\n","        ssim_list = []\n","        rrmse_list = []\n","        rpe_list = []\n","\n","    for it in range(iter):\n","        for batch_no, (batch_input, batch_target) in enumerate(fire_data_loader_train):\n","            model.train()\n","\n","            batch_input, batch_target = batch_input.to(device, dtype=torch.float32), batch_target.to(device, dtype=torch.int64)\n","\n","            optimizer.zero_grad()\n","\n","            logits = model(batch_input)\n","            # if batch_no % 100 == 0:\n","            #     print(torch.unique(batch_input),torch.unique(batch_target),torch.unique(torch.argmax(logits, dim=1)))\n","            #print(f'logit shape{logits.shape}')\n","            train_loss = criterion(logits, batch_target)\n","\n","            train_loss.backward()\n","            optimizer.step()\n","\n","            # if batch_no % 50 == 0:\n","            #     print('Batch:{:5d}  |  Train loss: {:.4f}'.format(batch_no, train_loss))\n","\n","            if test:\n","                if batch_no % 200 == 0:\n","\n","                    n_list=[21*random.randint(0,20)+14 for i in range(16)]\n","\n","                    input1 = []\n","                    target = []\n","\n","                    for n in n_list:\n","                        input1.append(test_data[n][0].numpy())\n","                        target.append(test_data[n+6][1].numpy())\n","\n","                    input1 = torch.tensor(input1)\n","                    target = torch.tensor(target)\n","                    input0 = input1.clone()\n","\n","                    # Validation\n","                    model.eval()\n","                    with torch.no_grad():\n","                        test_input_seq, test_input1_seq, test_target_seq = input0, input1, target\n","                        np_test_target_seq = test_target_seq.numpy()\n","                        test_input_seq, test_target_seq, test_input1_seq = test_input_seq.to(device, dtype=torch.float32), test_target_seq.to(device, dtype=torch.float32), test_input1_seq.to(device, dtype=torch.float32)\n","                        test_predicted_seq = loop_testing(test_input1_seq, 3, model)\n","                        np_test_predicted_seq = test_predicted_seq.cpu().numpy()\n","\n","                        mse_test_loss = criterion2(test_predicted_seq, test_target_seq)\n","                        mse_list.append(mse_test_loss)\n","\n","                        ssim_test_loss = ssim(np_test_target_seq, np_test_predicted_seq, data_range=np_test_predicted_seq.max() - np_test_predicted_seq.min(), channel_axis=1)\n","                        # test_loss3 = ssim(np_target12[i], np_input1[i], data_range=np_input1[i].max() - np_input1[i].min(), channel_axis=0)\n","                        ssim_list.append(ssim_test_loss)\n","\n","                        rrmse_test_loss = torch.sqrt((torch.sum(torch.tensor(np.square(np_test_target_seq - np_test_predicted_seq))) / batch_size)/torch.sum(torch.square(torch.tensor(np_test_predicted_seq))))\n","                        rrmse_list.append(rrmse_test_loss)\n","\n","                        rpe_test_loss = np.sum(np_test_target_seq != np_test_predicted_seq)/(128*128)\n","                        rpe_list.append(rpe_test_loss)\n","\n","                        print(\"Iter:{:5d}|Train loss: {:.4f}|Test loss MSE: {:.4f}| SSIM:{:.4f}| RRMSE:{:.4f}| RPE:{:.4f}\".format(batch_no, train_loss, mse_test_loss, ssim_test_loss, rrmse_test_loss, rpe_test_loss))\n","\n","\n","    if test:\n","        return model, mse_list, ssim_list, rrmse_list, rpe_list\n","    else:\n","        return model\n"]},{"cell_type":"markdown","metadata":{"id":"2pgfyqeAwTOT"},"source":["### data function"]},{"cell_type":"markdown","metadata":{"id":"x6POiQ16ZqFg"},"source":["#### part 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVOS7qwV1-Su"},"outputs":[],"source":["def loop_testing_all_ver1(input, loop):\n","\n","    ans_list = []\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    input = input.to(device, dtype=torch.float32)\n","    old_input = input.clone().unsqueeze(0)\n","\n","    for i in range(loop):\n","        print(f'i={i}')\n","        # new_input = input.colne()\n","\n","        output = convLSTM(old_input).clone().detach()\n","        #print(input.shape)\n","        output = torch.argmax(output[:,:3], dim=1)\n","        ans_list.extend(output.squeeze(0))\n","        #print(input.shape)\n","        output = torch.stack((output, output), dim=2)\n","\n","        old_input = output\n","\n","    return ans_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79iq5kLwysgd"},"outputs":[],"source":["def count_fire(forest):\n","    count_list=[]\n","\n","    i, m, n = np.shape(forest)\n","\n","    for i1 in range(i):\n","        count = 0\n","        for m1 in range(m):\n","            for n1 in range(n):\n","                if forest[i1][m1][n1] == 1:\n","                    count += 1\n","        count_list.append(count)\n","\n","    return count_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JdxcOjFk413t"},"outputs":[],"source":["def loop_testing(input1, loop, model):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    input1 = input1.to(device, dtype=torch.float32)\n","\n","    for i in range(loop-1):\n","        old_input = input1\n","        input1 = model(old_input).detach()\n","        input1 = torch.argmax(input1, dim=1)\n","        input1 = torch.stack((input1, input1), dim=2)\n","\n","    old_input = input1\n","    input1 = model(old_input).detach()\n","    input1 = torch.argmax(input1, dim=1)\n","\n","    return input1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDpeuuAuwTBC"},"outputs":[],"source":["# Function to filter out outliers based on IQR\n","def filter_outliers(data_column):\n","    Q1 = np.percentile(data_column, 25)\n","    Q3 = np.percentile(data_column, 75)\n","    IQR = Q3 - Q1\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","    return data_column[(data_column >= lower_bound) & (data_column <= upper_bound)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVqC9v-KyFrw"},"outputs":[],"source":["def loop_testing_all(input11, loop):\n","    ans_list = []\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    input11 = input11.to(device, dtype=torch.float32).unsqueeze(0)\n","\n","    for i in range(loop-1):\n","        old_input1 = input11.clone()  # Clone to avoid altering the original tensor\n","\n","        # Forward pass through the convLSTM model\n","        input11 = convLSTM(old_input1).clone().detach()\n","\n","        # Take the argmax along the class dimension\n","        input11 = torch.argmax(input11, dim=1)\n","\n","        # Store predictions\n","        ans_list.extend(input11.squeeze(0))  # Append without extending; keep sequence structure\n","\n","        # Prepare input for the next loop iteration\n","        input11 = input11.unsqueeze(1)  # Adjust dimensions if needed for the next input\n","\n","    old_input1 = input11.clone()\n","    #print(input.shape)\n","    input11 = convLSTM(old_input1).clone().detach()\n","    input11 = torch.argmax(input11, dim=1)\n","    ans_list.extend(input11.squeeze(0))\n","\n","    return ans_list\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWnyfQE_clwJ"},"outputs":[],"source":["def loop_testing_msessimrpe_1to1(test_data, m):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    input1, target1 = test_data[m]\n","    _, target3 = test_data[m+3]\n","    _, target6 = test_data[m+6]\n","    _, target9 = test_data[m+9]\n","    _, target12 = test_data[m+12]\n","    input0 = input1.clone()\n","\n","    input1 = input1.to(device, dtype=torch.float32).unsqueeze(0)\n","    target1 = target1.to(device, dtype=torch.float32).unsqueeze(0)\n","    target3 = target3.to(device, dtype=torch.float32).unsqueeze(0)\n","    target6 = target6.to(device, dtype=torch.float32).unsqueeze(0)\n","    target9 = target9.to(device, dtype=torch.float32).unsqueeze(0)\n","    target12 = target12.to(device, dtype=torch.float32).unsqueeze(0)\n","\n","    mse_list = []\n","    ssim_list = []\n","    rpe_list = []\n","    pre_fire_count = []\n","    tar_fire_count = []\n","\n","    time_list=[]\n","\n","    mse = nn.MSELoss()\n","\n","    #loop=1\n","    old_input = input1\n","    start = time.time()\n","    input1 = convLSTM(old_input).detach()\n","    time_list.append(time.time()-start)\n","    input1 = torch.argmax(input1, dim=1)\n","\n","    np_input1 = input1.cpu().squeeze(0).numpy()\n","    np_target1 = target1.cpu().squeeze(0).numpy()\n","\n","    for i in range(3):\n","        test_loss2 = mse(input1[0][i], target1[0][i])\n","        mse_list.append(test_loss2)\n","\n","    for i in range(3):\n","        test_loss3 = ssim(np_target1[i], np_input1[i], data_range=np_input1[i].max() - np_input1[i].min(), channel_axis=0)\n","        ssim_list.append(test_loss3)\n","\n","    for i in range(3):\n","        test_loss4 = np.sum(np_target1[i] != np_input1[i])/(128*128)\n","        rpe_list.append(test_loss4)\n","\n","    pre_fire_count.append(count_fire(np_input1))\n","    tar_fire_count.append(count_fire(np_target1))\n","\n","    input1 = torch.stack((input1, input1), dim=2)\n","\n","    #loop=2\n","    old_input = input1\n","    start = time.time()\n","    input1 = convLSTM(old_input).detach()\n","    time_list.append(time.time()-start)\n","    input1 = torch.argmax(input1, dim=1)\n","    np_input1 = input1.cpu().squeeze(0).numpy()\n","    np_target3 = target3.cpu().squeeze(0).numpy()\n","\n","    for i in range(3):\n","        test_loss2 = mse(input1[0][i], target3[0][i])\n","        mse_list.append(test_loss2)\n","\n","    for i in range(3):\n","        test_loss3 = ssim(np_target3[i], np_input1[i], data_range=np_input1[i].max() - np_input1[i].min(), channel_axis=0)\n","        ssim_list.append(test_loss3)\n","\n","    for i in range(3):\n","        test_loss4 = np.sum(np_target3[i] != np_input1[i])/(128*128)\n","        rpe_list.append(test_loss4)\n","\n","    pre_fire_count.append(count_fire(np_input1))\n","    tar_fire_count.append(count_fire(np_target3))\n","\n","    input1 = torch.stack((input1, input1), dim=2)\n","\n","    #loop=3\n","    old_input = input1\n","    #print(input.shape)\n","    start = time.time()\n","    input1 = convLSTM(old_input).detach()\n","    time_list.append(time.time()-start)\n","    input1 = torch.argmax(input1, dim=1)\n","    np_input1 = input1.cpu().squeeze(0).numpy()\n","    np_target6 = target6.cpu().squeeze(0).numpy()\n","\n","    for i in range(3):\n","        test_loss2 = mse(input1[0][i], target6[0][i])\n","        mse_list.append(test_loss2)\n","\n","    for i in range(3):\n","        test_loss3 = ssim(np_target6[i], np_input1[i], data_range=np_input1[i].max() - np_input1[i].min(), channel_axis=0)\n","        ssim_list.append(test_loss3)\n","\n","    for i in range(3):\n","        test_loss4 = np.sum(np_target6[i] != np_input1[i])/(128*128)\n","        rpe_list.append(test_loss4)\n","\n","    pre_fire_count.append(count_fire(np_input1))\n","    tar_fire_count.append(count_fire(np_target6))\n","\n","    input1 = torch.stack((input1, input1), dim=2)\n","\n","    #loop=4\n","    old_input = input1\n","    start = time.time()\n","    input1 = convLSTM(old_input).detach()\n","    time_list.append(time.time()-start)\n","    input1 = torch.argmax(input1, dim=1)\n","    np_input1 = input1.cpu().squeeze(0).numpy()\n","    np_target9 = target9.cpu().squeeze(0).numpy()\n","\n","\n","    for i in range(3):\n","        test_loss2 = mse(input1[0][i], target9[0][i])\n","        mse_list.append(test_loss2)\n","\n","    for i in range(3):\n","        test_loss3 = ssim(np_target9[i], np_input1[i], data_range=np_input1[i].max() - np_input1[i].min(), channel_axis=0)\n","        ssim_list.append(test_loss3)\n","\n","    for i in range(3):\n","        test_loss4 = np.sum(np_target9[i] != np_input1[i])/(128*128)\n","        rpe_list.append(test_loss4)\n","\n","    pre_fire_count.append(count_fire(np_input1))\n","    tar_fire_count.append(count_fire(np_target9))\n","\n","    input1 = torch.stack((input1, input1), dim=2)\n","\n","    #loop=5\n","    old_input = input1\n","    #print(input.shape)\n","    start = time.time()\n","    input1 = convLSTM(old_input).detach()\n","    time_list.append(time.time()-start)\n","    input1 = torch.argmax(input1, dim=1)\n","    np_input1 = input1.cpu().squeeze(0).numpy()\n","    np_target12 = target12.cpu().squeeze(0).numpy()\n","\n","    for i in range(3):\n","        test_loss2 = mse(input1[0][i], target12[0][i])\n","        mse_list.append(test_loss2)\n","\n","    for i in range(3):\n","        test_loss3 = ssim(np_target12[i], np_input1[i], data_range=np_input1[i].max() - np_input1[i].min(), channel_axis=0)\n","        ssim_list.append(test_loss3)\n","\n","    for i in range(3):\n","        test_loss4 = np.sum(np_target12[i] != np_input1[i])/(128*128)\n","        rpe_list.append(test_loss4)\n","\n","    pre_fire_count.append(count_fire(np_input1))\n","    tar_fire_count.append(count_fire(np_target12))\n","\n","    input1 = torch.argmax(input1, dim=1)\n","\n","    for i in range(len(mse_list)):\n","        mse_list[i] = float(mse_list[i].cpu())\n","\n","    return input0, input1, np.array(mse_list), np.array(ssim_list), np.array(rpe_list), pre_fire_count, tar_fire_count, np.mean(time_list)\n"]},{"cell_type":"markdown","metadata":{"id":"ZPjpwxDnZuA0"},"source":["#### part 2 error map"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNAA365C0-Te"},"outputs":[],"source":["def create_error_map(target, predict):\n","    \"\"\"\n","    Creates an error map from target (ground truth) and prediction arrays.\n","    0 for correct predictions,\n","    1 for false negatives (missed fires),\n","    2 for false positives (over-predicted fires).\n","    \"\"\"\n","    error_map = np.zeros_like(target, dtype=np.uint8)\n","\n","    # False Negatives (FN)\n","    error_map[(target == 1) & (predict == 0)] = 1\n","\n","    # False Positives (FP)\n","    error_map[(target == 0) & (predict == 1)] = 2\n","\n","    return error_map\n","\n","def save_custom_layout(input_images, target_images, prediction_images, error_images, image_directory, fire, loop_times, VERBOSE):\n","    total_columns = max(len(target_images), len(prediction_images), len(error_images))\n","    total_rows = 3  # Rows for Input, Target, Prediction, Error\n","\n","    fig, axes = plt.subplots(total_rows, total_columns, figsize=(total_columns * 3, total_rows * 3), gridspec_kw={'width_ratios': [1]*total_columns}, dpi=500)\n","\n","    # Helper to hide axes\n","    def hide_axes(ax):\n","        ax.axis('off')\n","\n","    # Function to add image titles\n","    def add_image_titles(ax, category, index):\n","        ax.set_title(f'{category} {2*(index + 1)}', fontsize=12)\n","\n","    # Titles for the rows\n","    # row_titles = ['Input', 'Target', 'Prediction', 'Error']\n","    row_titles = [ 'Target', 'Prediction', 'Error']\n","\n","    # Plot Images in each row\n","    # image_lists = [input_images, target_images, prediction_images, error_images]\n","    image_lists = [target_images, prediction_images, error_images]\n","    for row_idx, image_list in enumerate(image_lists):\n","        for col_idx in range(total_columns):\n","            if col_idx < len(image_list):\n","                cmap = error_cmap if row_idx == 2 else default_cmap\n","                norm = error_norm if row_idx == 2 else default_norm\n","                axes[row_idx, col_idx].imshow(image_list[col_idx], cmap=cmap, norm=norm)\n","                # if row_idx == 0:\n","                    # print(np.unique(image_list[col_idx]))\n","                add_image_titles(axes[row_idx, col_idx], row_titles[row_idx], col_idx)\n","            hide_axes(axes[row_idx, col_idx])\n","\n","    plt.tight_layout()\n","\n","    # Save the figure\n","    file_format = 'pdf'\n","\n","    if VERBOSE:\n","        plt.savefig(image_directory + f'.{file_format}', format=file_format, bbox_inches='tight')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qvi2thes7pJs"},"outputs":[],"source":["# Define the error colormap and norm\n","error_cmap = mpl.colors.ListedColormap(['lightgrey', 'red', 'blue'])\n","error_bounds = [-0.5, 0.5, 1.5, 2.5]\n","error_norm = mpl.colors.BoundaryNorm(error_bounds, error_cmap.N)"]},{"cell_type":"markdown","metadata":{"id":"w0Y7cXqfL7QQ"},"source":["### train model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DRZdnyj9mT5N"},"outputs":[],"source":["# chimney\n","\n","#chimney train model init\n","train_data = FireDataset(forest_chimney_train, seq_len=6)\n","validation_data = FireDataset(forest_chimney_val_4pt, seq_len=6)\n","convLSTM, mse_4pt_list, ssim_4pt_list, rrmse_4pt_list, rpe_4pt_list  = train_convLSTM(train_data, test=True, test_data=validation_data)\n","\n","mse_4pt_arr = np.array([tensor.to('cpu').numpy() for tensor in mse_4pt_list])\n","\n","# Save the NumPy array to a file\n","np.save(directory + 'Data/training_metric_data/chimney_mse_4pt_arr.npy', mse_4pt_arr)\n","\n","ssim_4pt_arr = np.array(ssim_4pt_list)\n","\n","# Save the NumPy array to a file\n","np.save(directory + 'Data/training_metric_data/chimney_ssim_4pt_arr.npy', ssim_4pt_arr)\n","rrmse_4pt_arr = np.array(rrmse_4pt_list)\n","\n","# Save the NumPy array to a file\n","np.save(directory + 'Data/training_metric_data/chimney_rrmse_4pt_arr.npy', rrmse_4pt_arr)\n","\n","rpe_4pt_arr = np.array(rpe_4pt_list)\n","\n","# Save the NumPy array to a file\n","np.save(directory + 'Data/training_metric_data/chimney_rpe_4pt_arr.npy', rpe_4pt_arr)\n","\n","print(rpe_4pt_arr.shape)\n","\n","#Save the model\n","os.makedirs(directory + 'Data/trained_models/', exist_ok=True)\n","model_path = os.path.join(directory + 'Data/trained_models/', 'convlstm_chimney_rand_ig_rand_wind_3t1p.pt')\n","torch.save(convLSTM.state_dict(), model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Q1y_k7GmjHs"},"outputs":[],"source":["# ferguson\n","\n","#ferguson train model init\n","train_data = FireDataset(forest_fer_train, seq_len=6)\n","validation_data = FireDataset(forest_fer_val_4pt, seq_len=6)\n","convLSTM, mse_4pt_list, ssim_4pt_list, rrmse_4pt_list, rpe_4pt_list  = train_convLSTM(train_data, test=True, test_data=validation_data)\n","\n","mse_4pt_arr = np.array([tensor.to('cpu').numpy() for tensor in mse_4pt_list])\n","\n","# Save the NumPy array to a file\n","np.save(directory + 'Data/training_metric_data/ferguson_mse_4pt_arr.npy', mse_4pt_arr)\n","\n","ssim_4pt_arr = np.array(ssim_4pt_list)\n","\n","# Save the NumPy array to a file\n","np.save(directory + 'Data/training_metric_data/ferguson_ssim_4pt_arr.npy', ssim_4pt_arr)\n","rrmse_4pt_arr = np.array(rrmse_4pt_list)\n","\n","# Save the NumPy array to a filewu wu\n","np.save(directory + 'Data/training_metric_data/ferguson_rrmse_4pt_arr.npy', rrmse_4pt_arr)\n","\n","rpe_4pt_arr = np.array(rpe_4pt_list)\n","\n","# Save the NumPy array to a file\n","np.save(directory + 'Data/training_metric_data/ferguson_rpe_4pt_arr.npy', rpe_4pt_arr)\n","\n","print(rpe_4pt_arr.shape)\n","\n","#Save the model\n","os.makedirs(directory + 'Data/trained_models/', exist_ok=True)\n","model_path = os.path.join(directory + 'Data/trained_models/', 'convlstm_ferguson_rand_ig_rand_wind_3t1p.pt')\n","torch.save(convLSTM.state_dict(), model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YYFW6d2nmg4P"},"outputs":[],"source":["# bear\n","mse_4pt_arr = np.array([tensor.to('cpu').numpy() for tensor in mse_4pt_list])\n","\n","# Save the NumPy array to a file\n","np.save(directory + 'Data/training_metric_data/bear_mse_4pt_arr.npy', mse_4pt_arr)\n","\n","ssim_4pt_arr = np.array(ssim_4pt_list)\n","\n","# Save the NumPy array to a file\n","np.save(directory + 'Data/training_metric_data/bear_ssim_4pt_arr.npy', ssim_4pt_arr)\n","rrmse_4pt_arr = np.array(rrmse_4pt_list)\n","\n","# Save the NumPy array to a file\n","np.save(directory + 'Data/training_metric_data/bear_rrmse_4pt_arr.npy', rrmse_4pt_arr)\n","\n","rpe_4pt_arr = np.array(rpe_4pt_list)\n","\n","# Save the NumPy array to a file\n","np.save(directory + 'Data/training_metric_data/bear_rpe_4pt_arr.npy', rpe_4pt_arr)\n","\n","rpe_4pt_arr.shape\n","\n","#Save the model\n","os.makedirs(directory + 'Data/trained_models/', exist_ok=True)\n","model_path = os.path.join(directory + 'Data/trained_models/', 'convlstm_bear_rand_ig_rand_wind_3t1p.pt')\n","torch.save(convLSTM.state_dict(), model_path)"]},{"cell_type":"markdown","metadata":{"id":"ev24CyU_CDAP"},"source":["### test on Ferguson\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rll9x8qrCH0e"},"outputs":[],"source":["# Load the model from the .pt file\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","input_dim = 2\n","hidden_dim = 128\n","convLSTM = EncoderDecoderConvLSTM(input_dim, hidden_dim).to(device)\n","model_path = os.path.join(directory + 'Data/trained_models/', 'convlstm_ferguson_rand_ig_rand_wind_3t1p.pt')\n","convLSTM.load_state_dict(torch.load(model_path))\n","\n","# Set the model to evaluation mode (if needed)\n","convLSTM.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-GMOpVf1Fvj"},"outputs":[],"source":["test_data_tilt_4_ptbar = FireDataset(forest_fer_test_4pt, seq_len=6)\n","test_data_tilt_no_ptbar = FireDataset(forest_fer_test_no_bar, seq_len=6)\n","test_data_tilt_3_tbar = FireDataset(forest_fer_test_3t, seq_len=6)\n","test_data_tilt_2_pbar = FireDataset(forest_fer_test_2p, seq_len=6)\n","test_data_tilt_2_p_longtime_bar = FireDataset(forest_fer_test_2p_longtime, seq_len=6)"]},{"cell_type":"code","source":["# 2p version\n","\n","# Main processing\n","loop_times = 5\n","good_n = [90]\n","n = [x for x in range (90,100)]\n","fire = 'ferguson'\n","VERBOSE = True\n","\n","# Example usage in your main loop\n","for n1 in good_n:\n","    n1 = n1\n","    print(f'n={n1}')\n","    m = 76 * n1 + 60\n","    image_directory = directory + 'Plots/test_image_2p/{fire}_loop{loop_times}_{n1}'\n","\n","    input11, _ = test_data_tilt_2_p_longtime_bar[m]\n","    input01 = input11.clone()\n","    predicted_labels_list = loop_testing_all_ver1(input11, loop=loop_times)\n","\n","    # Prepare images\n","    input_images = [input01[i, 0].cpu().numpy() + forest for i in range(3)]\n","    target_images = []\n","    prediction_images = []\n","    error_images = []\n","\n","    for i in range((loop_times) * 3):\n","        np_pre = predicted_labels_list[i].cpu().numpy()\n","        _, target1 = test_data_tilt_2_p_longtime_bar[m + i]\n","        target_np = target1[0].cpu().numpy()\n","        error_map = create_error_map(target_np, np_pre)\n","\n","        if i % 2 == 1:\n","            target_images.append(target_np + forest)\n","            prediction_images.append(np_pre + forest)\n","            error_images.append(error_map)\n","\n","    # Call the custom function to save the layout\n","    # print(np.unique(target_images[0]))\n","    save_custom_layout(input_images, target_images, prediction_images, error_images, image_directory, fire, loop_times, VERBOSE =  VERBOSE)\n"],"metadata":{"id":"iyceWMMaKgjV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T95On0PViuUq"},"outputs":[],"source":["# Main processing\n","loop_times = 5\n","good_n = [35]\n","n = [x for x in range(0,11)]\n","fire = 'ferguson'\n","VERBOSE = False\n","\n","# Example usage in your main loop\n","for n1 in n:\n","    n1 = n1\n","    print(f'n={n1}')\n","    m = 21 * n1 + 6\n","    image_directory = directory + 'Plots/test_image/{fire}_loop{loop_times}_{n1}'\n","\n","    input11, _ = test_data_tilt_4_ptbar[m]\n","    input01 = input11.clone()\n","    predicted_labels_list = loop_testing_all_ver1(input11, loop=loop_times)\n","\n","    # Prepare images\n","    input_images = [input01[i, 0].cpu().numpy() + forest for i in range(3)]\n","    target_images = []\n","    prediction_images = []\n","    error_images = []\n","\n","    for i in range((loop_times) * 3):\n","        np_pre = predicted_labels_list[i].cpu().numpy()\n","        _, target1 = test_data_tilt_4_ptbar[m + i]\n","        target_np = target1[0].cpu().numpy()\n","        error_map = create_error_map(target_np, np_pre)\n","\n","        if i % 2 == 1:\n","            target_images.append(target_np + forest)\n","            prediction_images.append(np_pre + forest)\n","            error_images.append(error_map)\n","\n","    # Call the custom function to save the layout\n","    save_custom_layout(input_images, target_images, prediction_images, error_images, image_directory, fire, loop_times, VERBOSE = VERBOSE)\n"]},{"cell_type":"markdown","metadata":{"id":"y7MCZxp-NRuR"},"source":["#### Testing: MSE, SSIM and RPE on senarios of 3T1P & no bar & 3T $ 2P"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8gaPMtbNRuR"},"outputs":[],"source":["mse_2_p_list = []\n","ssim_2_p_list = []\n","rpe_2_p_list = []\n","pre_count_2_p_list = []\n","tar_count_2_p_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_2_pbar, m)\n","    mse_2_p_list.append(mse_arr)\n","    ssim_2_p_list.append(ssim_arr)\n","    rpe_2_p_list.append(rpe_arr)\n","    pre_count_2_p_list.append(pre_fire_count)\n","    tar_count_2_p_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oo4gc4NINRuR"},"outputs":[],"source":["mse_2_p_array = np.array(mse_2_p_list)\n","np.save(directory + 'Data/test_metric_data/Ferguson_data/mse_2_p_array.npy', mse_2_p_array)\n","\n","ssim_2_p_array = np.array(ssim_2_p_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/ssim_2_p_array.npy', ssim_2_p_array)\n","\n","rpe_2_p_array = np.array(rpe_2_p_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/rpe_2_p_array.npy', rpe_2_p_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDbNCxcFNRuR"},"outputs":[],"source":["mse_3_t_list = []\n","ssim_3_t_list = []\n","rpe_3_t_list = []\n","pre_count_3_t_list = []\n","tar_count_3_t_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_3_tbar, m)\n","    mse_3_t_list.append(mse_arr)\n","    ssim_3_t_list.append(ssim_arr)\n","    rpe_3_t_list.append(rpe_arr)\n","    pre_count_3_t_list.append(pre_fire_count)\n","    tar_count_3_t_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLpp4T8nNRuS"},"outputs":[],"source":["mse_3_t_array = np.array(mse_3_t_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/mse_3_t_array.npy', mse_3_t_array)\n","\n","ssim_3_t_array = np.array(ssim_3_t_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/ssim_3_t_array.npy', ssim_3_t_array)\n","\n","rpe_3_t_array = np.array(rpe_3_t_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/rpe_3_t_array.npy', rpe_3_t_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rv6zkMToNRuS"},"outputs":[],"source":["mse_4_pt_list = []\n","ssim_4_pt_list = []\n","rpe_4_pt_list = []\n","pre_count_4_pt_list = []\n","tar_count_4_pt_list = []\n","mean_time_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_4_ptbar, m)\n","    mse_4_pt_list.append(mse_arr)\n","    ssim_4_pt_list.append(ssim_arr)\n","    rpe_4_pt_list.append(rpe_arr)\n","    pre_count_4_pt_list.append(pre_fire_count)\n","    tar_count_4_pt_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eS-QMR5tNRuS"},"outputs":[],"source":["mse_4_pt_array = np.array(mse_4_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/mse_4_pt_array.npy', mse_4_pt_array)\n","\n","ssim_4_pt_array = np.array(ssim_4_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/ssim_4_pt_array.npy', ssim_4_pt_array)\n","\n","rpe_4_pt_array = np.array(rpe_4_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/rpe_4_pt_array.npy', rpe_4_pt_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsWCiLfENRuS"},"outputs":[],"source":["mse_no_pt_list = []\n","ssim_no_pt_list = []\n","rpe_no_pt_list = []\n","pre_count_no_pt_list = []\n","tar_count_no_pt_list = []\n","mean_time_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_no_ptbar, m)\n","    mse_no_pt_list.append(mse_arr)\n","    ssim_no_pt_list.append(ssim_arr)\n","    rpe_no_pt_list.append(rpe_arr)\n","    pre_count_no_pt_list.append(pre_fire_count)\n","    tar_count_no_pt_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7KWketYNRuS"},"outputs":[],"source":["mse_no_pt_array = np.array(mse_no_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/mse_no_pt_array.npy', mse_no_pt_array)\n","\n","ssim_no_pt_array = np.array(ssim_no_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/ssim_no_pt_array.npy', ssim_no_pt_array)\n","\n","rpe_no_pt_array = np.array(rpe_no_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/rpe_no_pt_array.npy', rpe_no_pt_array)"]},{"cell_type":"markdown","metadata":{"id":"0RKB0hpBb9WT"},"source":["### test on chimney"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkObEmrDb9Wi"},"outputs":[],"source":["# Load the model from the .pt file\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","input_dim = 2\n","hidden_dim = 128\n","convLSTM = EncoderDecoderConvLSTM(input_dim, hidden_dim).to(device)\n","model_path = os.path.join(directory + 'Data/trained_models/', 'convlstm_chimney_rand_ig_rand_wind_3t1p.pt')\n","convLSTM.load_state_dict(torch.load(model_path))\n","\n","# Set the model to evaluation mode (if needed)\n","convLSTM.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaJI8xd5b9Wj"},"outputs":[],"source":["test_data_tilt_4_ptbar = FireDataset(forest_chimney_test_4pt, seq_len=6)\n","test_data_tilt_no_ptbar = FireDataset(forest_chimney_test_no_bar, seq_len=6)\n","test_data_tilt_3_tbar = FireDataset(forest_chimney_test_3t, seq_len=6)\n","test_data_tilt_2_pbar = FireDataset(forest_chimney_test_2p, seq_len=6)"]},{"cell_type":"code","source":["# 2p version\n","\n","# Main processing\n","loop_times = 5\n","good_n = [36]\n","n = [x for x in range (30,60)]\n","fire = 'chimney'\n","VERBOSE = True\n","\n","# Example usage in your main loop\n","for n1 in good_n:\n","    n1 = n1\n","    print(f'n={n1}')\n","    m = 21 * n1 + 6\n","    image_directory = directory + 'Plots/test_image_2p/{fire}_loop{loop_times}_{n1}'\n","\n","    input11, _ = test_data_tilt_2_pbar[m]\n","    input01 = input11.clone()\n","    predicted_labels_list = loop_testing_all_ver1(input11, loop=loop_times)\n","\n","    # Prepare images\n","    input_images = [input01[i, 0].cpu().numpy() + forest for i in range(3)]\n","    target_images = []\n","    prediction_images = []\n","    error_images = []\n","\n","    for i in range((loop_times) * 3):\n","        np_pre = predicted_labels_list[i].cpu().numpy()\n","        _, target1 = test_data_tilt_2_pbar[m + i]\n","        target_np = target1[0].cpu().numpy()\n","        error_map = create_error_map(target_np, np_pre)\n","\n","        if i % 2 == 1:\n","            target_images.append(target_np + forest)\n","            prediction_images.append(np_pre + forest)\n","            error_images.append(error_map)\n","\n","    # Call the custom function to save the layout\n","    # print(np.unique(target_images[0]))\n","    save_custom_layout(input_images, target_images, prediction_images, error_images, image_directory, fire, loop_times, VERBOSE =  VERBOSE)\n"],"metadata":{"id":"3RTCUcU4OEtC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0g5zIGkwHGk"},"outputs":[],"source":["# Main processing\n","loop_times = 5\n","good_n = [25]\n","n = [40,41,42,43,44,45,46,47,48,49]\n","fire = 'chimney'\n","VERBOSE = True\n","\n","# Example usage in your main loop\n","for n1 in good_n:\n","    n1 = n1\n","    print(f'n={n1}')\n","    m = 21 * n1 + 6\n","    image_directory = directory + 'Plots/test_image/{fire}_loop{loop_times}_{n1}'\n","\n","\n","    input11, _ = test_data_tilt_4_ptbar[m]\n","    input01 = input11.clone()\n","    predicted_labels_list = loop_testing_all_ver1(input11, loop=loop_times)\n","\n","    # Prepare images\n","    input_images = [input01[i, 0].cpu().numpy() + forest for i in range(3)]\n","    target_images = []\n","    prediction_images = []\n","    error_images = []\n","\n","    for i in range((loop_times) * 3):\n","        np_pre = predicted_labels_list[i].cpu().numpy()\n","        _, target1 = test_data_tilt_4_ptbar[m + i]\n","        target_np = target1[0].cpu().numpy()\n","        error_map = create_error_map(target_np, np_pre)\n","\n","        if i % 2 == 1:\n","            target_images.append(target_np + forest)\n","            prediction_images.append(np_pre + forest)\n","            error_images.append(error_map)\n","\n","    # Call the custom function to save the layout\n","    save_custom_layout(input_images, target_images, prediction_images, error_images, image_directory, fire, loop_times, VERBOSE =  VERBOSE)\n"]},{"cell_type":"markdown","metadata":{"id":"Re69GkGTb9Wj"},"source":["#### Testing: MSE, SSIM and RPE on senarios of 3T1P & no bar & 3T $ 2P"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8isVahyCE1bN"},"outputs":[],"source":["mse_2_p_list = []\n","ssim_2_p_list = []\n","rpe_2_p_list = []\n","pre_count_2_p_list = []\n","tar_count_2_p_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_2_pbar, m)\n","    mse_2_p_list.append(mse_arr)\n","    ssim_2_p_list.append(ssim_arr)\n","    rpe_2_p_list.append(rpe_arr)\n","    pre_count_2_p_list.append(pre_fire_count)\n","    tar_count_2_p_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87v3-hn7qNmO"},"outputs":[],"source":["mse_2_p_array = np.array(mse_2_p_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/mse_2_p_array.npy', mse_2_p_array)\n","\n","ssim_2_p_array = np.array(ssim_2_p_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/ssim_2_p_array.npy', ssim_2_p_array)\n","\n","rpe_2_p_array = np.array(rpe_2_p_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/rpe_2_p_array.npy', rpe_2_p_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ai7kKVX-6-rT"},"outputs":[],"source":["mse_3_t_list = []\n","ssim_3_t_list = []\n","rpe_3_t_list = []\n","pre_count_3_t_list = []\n","tar_count_3_t_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_3_tbar, m)\n","    mse_3_t_list.append(mse_arr)\n","    ssim_3_t_list.append(ssim_arr)\n","    rpe_3_t_list.append(rpe_arr)\n","    pre_count_3_t_list.append(pre_fire_count)\n","    tar_count_3_t_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yov4DOPEqc5N"},"outputs":[],"source":["mse_3_t_array = np.array(mse_3_t_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/mse_3_t_array.npy', mse_3_t_array)\n","\n","ssim_3_t_array = np.array(ssim_3_t_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/ssim_3_t_array.npy', ssim_3_t_array)\n","\n","rpe_3_t_array = np.array(rpe_3_t_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/rpe_3_t_array.npy', rpe_3_t_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BF5Jxp_eb9Wj"},"outputs":[],"source":["mse_4_pt_list = []\n","ssim_4_pt_list = []\n","rpe_4_pt_list = []\n","pre_count_4_pt_list = []\n","tar_count_4_pt_list = []\n","mean_time_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_4_ptbar, m)\n","    mse_4_pt_list.append(mse_arr)\n","    ssim_4_pt_list.append(ssim_arr)\n","    rpe_4_pt_list.append(rpe_arr)\n","    pre_count_4_pt_list.append(pre_fire_count)\n","    tar_count_4_pt_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nlzpfYmZsoP-"},"outputs":[],"source":["mse_4_pt_array = np.array(mse_4_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/mse_4_pt_array.npy', mse_4_pt_array)\n","\n","ssim_4_pt_array = np.array(ssim_4_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/ssim_4_pt_array.npy', ssim_4_pt_array)\n","\n","rpe_4_pt_array = np.array(rpe_4_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/rpe_4_pt_array.npy', rpe_4_pt_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXy7VhXXb9Wk"},"outputs":[],"source":["mse_no_pt_list = []\n","ssim_no_pt_list = []\n","rpe_no_pt_list = []\n","pre_count_no_pt_list = []\n","tar_count_no_pt_list = []\n","mean_time_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_no_ptbar, m)\n","    mse_no_pt_list.append(mse_arr)\n","    ssim_no_pt_list.append(ssim_arr)\n","    rpe_no_pt_list.append(rpe_arr)\n","    pre_count_no_pt_list.append(pre_fire_count)\n","    tar_count_no_pt_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRkTC70Rp2up"},"outputs":[],"source":["mse_no_pt_array = np.array(mse_no_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/mse_no_pt_array.npy', mse_no_pt_array)\n","\n","ssim_no_pt_array = np.array(ssim_no_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/ssim_no_pt_array.npy', ssim_no_pt_array)\n","\n","rpe_no_pt_array = np.array(rpe_no_pt_list)\n","np.save(directory + 'Data/test_metric_data/Chimney_data/rpe_no_pt_array.npy', rpe_no_pt_array)"]},{"cell_type":"markdown","metadata":{"id":"iB7DhB440WMZ"},"source":["### test on bear"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HU7w2Ylo0WMa"},"outputs":[],"source":["# Load the model from the .pt file\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","input_dim = 2\n","hidden_dim = 128\n","convLSTM = EncoderDecoderConvLSTM(input_dim, hidden_dim).to(device)\n","model_path = os.path.join(directory + 'Data/trained_models/', 'convlstm_bear_rand_ig_rand_wind_3t1p.pt')\n","convLSTM.load_state_dict(torch.load(model_path))\n","\n","# Set the model to evaluation mode\n","convLSTM.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pyZ745tX0WMa"},"outputs":[],"source":["test_data_tilt_4_ptbar = FireDataset(forest_bear_test_4pt, seq_len=6)\n","test_data_tilt_no_ptbar = FireDataset(forest_bear_test_no_bar, seq_len=6)\n","test_data_tilt_3_tbar = FireDataset(forest_bear_test_3t, seq_len=6)\n","test_data_tilt_2_pbar = FireDataset(forest_bear_test_2p, seq_len=6)\n","test_data_tilt_2_p_longtime_bar = FireDataset(forest_bear_test_2p_longtime, seq_len=6)"]},{"cell_type":"code","source":["# 2p version\n","\n","# Main processing\n","loop_times = 5\n","good_n = [90]\n","n = [x for x in range (0,50)]\n","fire = 'bear'\n","VERBOSE = False\n","\n","# Example usage in your main loop\n","for n1 in n:\n","    n1 = n1\n","    print(f'n={n1}')\n","    m = 76 * n1 + 60\n","    image_directory = directory + 'Plots/test_image_2p/{fire}_loop{loop_times}_{n1}'\n","\n","    input11, _ = test_data_tilt_2_p_longtime_bar[m]\n","    input01 = input11.clone()\n","    predicted_labels_list = loop_testing_all_ver1(input11, loop=loop_times)\n","\n","    # Prepare images\n","    input_images = [input01[i, 0].cpu().numpy() + forest for i in range(3)]\n","    target_images = []\n","    prediction_images = []\n","    error_images = []\n","\n","    for i in range((loop_times) * 3):\n","        np_pre = predicted_labels_list[i].cpu().numpy()\n","        _, target1 = test_data_tilt_2_p_longtime_bar[m + i]\n","        target_np = target1[0].cpu().numpy()\n","        error_map = create_error_map(target_np, np_pre)\n","\n","        if i % 2 == 1:\n","            target_images.append(target_np + forest)\n","            prediction_images.append(np_pre + forest)\n","            error_images.append(error_map)\n","\n","    # Call the custom function to save the layout\n","    # print(np.unique(target_images[0]))\n","    save_custom_layout(input_images, target_images, prediction_images, error_images, image_directory, fire, loop_times, VERBOSE =  VERBOSE)\n"],"metadata":{"id":"ZgYRGIe-G1fj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5m6HIKFlikh2"},"outputs":[],"source":["# 4pt version\n","\n","# Main processing\n","loop_times = 5\n","good_n = [28]\n","n = [40,41,42,43,44,45,46,47,48,49]\n","fire = 'bear'\n","VERBOSE = True\n","\n","# Example usage in your main loop\n","for n1 in good_n:\n","    n1 = n1\n","    print(f'n={n1}')\n","    m = 21 * n1 + 6\n","    image_directory = directory + 'Plots/test_image/{fire}_loop{loop_times}_{n1}'\n","\n","    input11, _ = test_data_tilt_4_ptbar[m]\n","    input01 = input11.clone()\n","    predicted_labels_list = loop_testing_all_ver1(input11, loop=loop_times)\n","\n","    # Prepare images\n","    input_images = [input01[i, 0].cpu().numpy() + forest for i in range(3)]\n","    target_images = []\n","    prediction_images = []\n","    error_images = []\n","\n","    for i in range((loop_times) * 3):\n","        np_pre = predicted_labels_list[i].cpu().numpy()\n","        _, target1 = test_data_tilt_4_ptbar[m + i]\n","        target_np = target1[0].cpu().numpy()\n","        error_map = create_error_map(target_np, np_pre)\n","\n","        if i % 2 == 1:\n","            target_images.append(target_np + forest)\n","            prediction_images.append(np_pre + forest)\n","            error_images.append(error_map)\n","\n","    # Call the custom function to save the layout\n","    # print(np.unique(target_images[0]))\n","    save_custom_layout(input_images, target_images, prediction_images, error_images, image_directory, fire, loop_times, VERBOSE =  VERBOSE)\n"]},{"cell_type":"markdown","metadata":{"id":"YfgUurTh0WMa"},"source":["#### Testing: MSE, SSIM and RPE on senarios of 3T1P & no bar & 3T $ 2P"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSrqPQrf0WMb"},"outputs":[],"source":["mse_2_p_list = []\n","ssim_2_p_list = []\n","rpe_2_p_list = []\n","pre_count_2_p_list = []\n","tar_count_2_p_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_2_pbar, m)\n","    mse_2_p_list.append(mse_arr)\n","    ssim_2_p_list.append(ssim_arr)\n","    rpe_2_p_list.append(rpe_arr)\n","    pre_count_2_p_list.append(pre_fire_count)\n","    tar_count_2_p_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OudnizC00WMb"},"outputs":[],"source":["mse_2_p_array = np.array(mse_2_p_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/mse_2_p_array.npy', mse_2_p_array)\n","\n","ssim_2_p_array = np.array(ssim_2_p_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/ssim_2_p_array.npy', ssim_2_p_array)\n","\n","rpe_2_p_array = np.array(rpe_2_p_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/rpe_2_p_array.npy', rpe_2_p_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nwqqQKQb0WMb"},"outputs":[],"source":["mse_3_t_list = []\n","ssim_3_t_list = []\n","rpe_3_t_list = []\n","pre_count_3_t_list = []\n","tar_count_3_t_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_3_tbar, m)\n","    mse_3_t_list.append(mse_arr)\n","    ssim_3_t_list.append(ssim_arr)\n","    rpe_3_t_list.append(rpe_arr)\n","    pre_count_3_t_list.append(pre_fire_count)\n","    tar_count_3_t_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-mrn5YG0WMb"},"outputs":[],"source":["mse_3_t_array = np.array(mse_3_t_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/mse_3_t_array.npy', mse_3_t_array)\n","\n","ssim_3_t_array = np.array(ssim_3_t_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/ssim_3_t_array.npy', ssim_3_t_array)\n","\n","rpe_3_t_array = np.array(rpe_3_t_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/rpe_3_t_array.npy', rpe_3_t_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zbUXRkLj0WMb"},"outputs":[],"source":["mse_4_pt_list = []\n","ssim_4_pt_list = []\n","rpe_4_pt_list = []\n","pre_count_4_pt_list = []\n","tar_count_4_pt_list = []\n","mean_time_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_4_ptbar, m)\n","    mse_4_pt_list.append(mse_arr)\n","    ssim_4_pt_list.append(ssim_arr)\n","    rpe_4_pt_list.append(rpe_arr)\n","    pre_count_4_pt_list.append(pre_fire_count)\n","    tar_count_4_pt_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIW9fOFt0WMb"},"outputs":[],"source":["mse_4_pt_array = np.array(mse_4_pt_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/mse_4_pt_array.npy', mse_4_pt_array)\n","\n","ssim_4_pt_array = np.array(ssim_4_pt_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/ssim_4_pt_array.npy', ssim_4_pt_array)\n","\n","rpe_4_pt_array = np.array(rpe_4_pt_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/rpe_4_pt_array.npy', rpe_4_pt_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceiokBZi0WMb"},"outputs":[],"source":["mse_no_pt_list = []\n","ssim_no_pt_list = []\n","rpe_no_pt_list = []\n","pre_count_no_pt_list = []\n","tar_count_no_pt_list = []\n","mean_time_list = []\n","\n","for n in tqdm(range(64)):\n","    m = 21*n + 6\n","    _, _, mse_arr, ssim_arr, rpe_arr, pre_fire_count, tar_fire_count, mean_time = loop_testing_msessimrpe_1to1(test_data_tilt_no_ptbar, m)\n","    mse_no_pt_list.append(mse_arr)\n","    ssim_no_pt_list.append(ssim_arr)\n","    rpe_no_pt_list.append(rpe_arr)\n","    pre_count_no_pt_list.append(pre_fire_count)\n","    tar_count_no_pt_list.append(tar_fire_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7Jzdx1c0WMc"},"outputs":[],"source":["mse_no_pt_array = np.array(mse_no_pt_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/mse_no_pt_array.npy', mse_no_pt_array)\n","\n","ssim_no_pt_array = np.array(ssim_no_pt_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/ssim_no_pt_array.npy', ssim_no_pt_array)\n","\n","rpe_no_pt_array = np.array(rpe_no_pt_list)\n","np.save(directory + 'Data/test_metric_data/Bear_data/rpe_no_pt_array.npy', rpe_no_pt_array)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["TMxAZF9_1ace","ZPjpwxDnZuA0","w0Y7cXqfL7QQ","ev24CyU_CDAP","y7MCZxp-NRuR","0RKB0hpBb9WT","Re69GkGTb9Wj","iB7DhB440WMZ","YfgUurTh0WMa"],"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNrmUR8SlCI1i5tALxKdfNf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}